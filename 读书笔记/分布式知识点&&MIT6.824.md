# 分布式系统

## 1.什么是分布式系统？

>  一个分布式系统由多个通过网络互联的独立自治的计算节点组成，这些计算节点基于消息传递机制进行相互协作，以完成共同的目标。

- 计算节点：计算机、进线程
- 独立自治：独立的CPU、独立的时钟、发生错误的时机和模式也相互独立
- 网络互连：以有线、无线等任意网络通信方式互连。不要求拓扑结构
- 并发：多结点动作同时进行。
- 消息传递：非内存共享。

## 2.什么是分布式计算

> 多个通过网络互联的计算节点通过相互协作共同完成计算任务。

## 3.什么是并行计算

> 并行计算包括分布式计算

不同层次的并行计算：

- 指令级并行：多指令并行；单指多数并行（向量指令）

- CPU多核并行：多线程编程

- 多CPU并行（一致性内存访问）：多线程编程

- 多CPU并行（非一致性内存访问）：超级计算机

- 基于GPU的并行：单指多数并行；CUDA、OpenCL

- 多机并行：基于消息传递的分布式计算（share nothing）

## 4.分布一致性协议（共识算法）

> paxos、raft

## 5.分布式系统：一致性

### 5.0 番外：数据库ACID中的一致性

- 传统ACID中的一致性：数据库从一个一致性的状态转移到另一个一致性的状态。这里的一致性表达的是数据库处于一种符合数据库约束，也符合用户预期的一种状态，其一方面依赖数据库的保证，例如原子性，也依赖于业务特性和业务层代码实现。（这种一致性既适用于单机数据库也适用于分布式数据库）

### 5.1 分布式数据一致性

> CAP中的一致性和共识算法（raft、poxas）的一致性都可以算作是分布式副本的一致性，其描述的是在数据副本的复制过程中呈现的一致性。

- 强一致性（线性一致性/原子一致性）（Linearizability）：
	- 任何用户看到的系统操作顺序，都和**全局时钟下的顺序一致**。
	- 任何一次读都能读到某个数据的最近一次写的结果。
- 顺序一致性（Sequential Consistency）：
	- 任何一次读都能读到某个数据的最近一次写的结果。
	- 系统的所有进程的顺序一致，而且是合理的。不需要和全局时钟下的顺序一致。(符合程序的先后顺序)
- 因果一致性：对有因果关系的事件有顺序要求。
- 最终一致性：系统中的所有分散在不同节点的数据，经过一定时间后，最终能够达到符合业务定义的一致的状态。

### 5.2 分布式事务一致性

- 外部一致性：事务 T1 执行完成后，T2 才启动，保证让 T2 看到 T1 的结果。
- 内部一致性：多个并发事务读写的数据（或数据范围），它们能够被数据库感知到冲突（读写之间也算冲突），因此它们在时间线上的顺序由数据库确定。

### 5.3 不知道怎么分类，也不知道和5.2的关系

- 可串行化：数据库的概念。对并发事务包含的操作进行调度后的结果和某种把这些事务一个接一个的执行之后的结果一样。
- 可线性化：分布式的概念。针对单个操作，单个数据对象而说的。一个数据被更新后，能够立马被后续的读操作读到。
- 严格可串行化: 同时满足Serializability和Linearizability。

## 6.并发和并行

并发：针对单核 CPU 而言，它指的是 CPU 交替执行不同任务的能力；

并行：针对多核 CPU 而言，它指的是多个核心同时执行多个任务的能力。

## 7.负载均衡策略

随机、轮询、固定权重、IP哈希、最少TCP连接数、最小响应时间和基于各服务器实际负载的动态负载均衡算法。

## 8.分布式系统架构

C/S、主-从、总线、对等。

## 9.分布式算法设计

设计分布式算法时使用的抽象分布式系统模型，从以下三个方面进行定义：

- 交互模式

	- 算法运行期间哪些节点之间有消息传递动作；

	- 通信模式：同步模式、异步模式

- 信道故障模式

	- 消息是否会丢失

	- 消息是否会乱序

	- 消息传输延迟的上限

- 节点故障模式

	- 失效停止模式（Fail-Stop） 

	- 失效停止重启模式（Fail-Stop-Recover） 

	- 拜占庭(Byzantine)模式

## 10.Socket

> 传输层和网络层提供给应用层的标准化编程接口

## 11.并发服务技术

- 基于多线程和线程池
- 事件驱动（多路复用）

## 12.RPC和RMI

> Remote Rrocedure Call & Remote Method Invocation

- 远程过程调用：使应用程序可以像调用本地节点上的过程(子程序) 那样去调用一个远程节点上的子程序。（一般采用同步技术）
- 远程方法调用：使应用程序可以像调用本机上对象的方法一样调用远程主机中对象的方法。
	- RPC/RMI中间件在调用者进程中植入stub/proxy模块，stub模块作为远程过程的本地代理，并且暴露与远程过程相同的接口。
	- RPC/RMI中间件在被调用者进程中植入skeleton模块，skeleton作为调用者在远程主机中的代理，代替客户端调用本地方法，并把结果返回给客户端。
	- stub模块与skeleton模块利用Socket进行通信。

## 13.Web Service

> 为方便网络上不同节点之间互操作而定义的一套协议标准，也可视为实现远程过程调用的一套协议标准。

<img src="https://raw.githubusercontent.com/JiXuanYu0823/ReadingNotes/main/assets/WebService%E5%8D%8F%E8%AE%AE%E6%A0%88.png" alt="WebService协议栈" style="zoom: 50%;" />

## 14.消息中间件

**消息队列** 是指利用 **高效可靠** 的 **消息传递机制** 进行与平台无关的 **数据交流**，并基于 **数据通信** 来进行分布式系统的集成。

- 点对点模型
- 发布/订阅

## 15.分布式存储的目标

- 提高数据存储容量
- 提高数据吞吐量
- 提高可靠性/可用性
- 降低数据延迟
- 提升分布式数据处理系统的运行效率

## 16.分布式存储的两种基本手段

- Replication：在多个不同的节点上保存相同数据的多个副本。

- 分区：将一个大型数据文件(或数据库)拆分成较小的子集（ 称为分区partition或切片shard ） ，再将不同的分区指派给不同的节点

## 17.主从复制

- 无主复制：
  - 客户端直接将写入请求发送到到各个副本；或者客户端将写入发送给某个节点，由该节点充当代理节点向其他节点转发写入请求。
- 单主复制：
	- 优点：适合读多写少的场景。
	- 缺点：有单点故障和脑裂问题。
- 多主复制：
	- 优点：多主库接受写操作，并发送给其他节点。写性能高。
	- 缺点：多副本一致性问题解决方案太复杂。

## 18.同步复制和异步复制

- 同步复制：领导者写入自己的存储器，并且接收到所有跟随者已经成功更新本地存储的应答后再向用户返回成功写入应答。
	- 优点：更容易实现副本之间的一致性；
	-  缺点：写入速度慢；一个节点失效会使整体写入功能失败
- 异步复制：领导者自己写入成功后立即向向用户返回成功应答，不等待其他跟随者的应答消息。
	- 优点：写入速度快；容错性好；
	- 缺点：保持多副本一致性复杂；
- 链式复制
- 混合复制

## 19.从节点数据更新

- 主节点发送给从节点的更新命令级别：
	- SQL命令：不是原子操作、容易造成不一致。解释SQL也浪费计算资源
	- CPU机器指令：效率太低
	- DB **更新日志** 中的数据更新指令：效率和复杂度的折中，容易实现原子操作。
- 更新日志：数据副本节点一般分两步处理数据写入(或复制)请求：
	- 先将更新操作信息**追加到更新日志**（Redo Log/Undo Log）；
	- 再根据操作参数**更新本地存储系统**（或称为本地状态机）。

## 20.复制状态机和状态转移

- 复制状态机：传输的是来自客户端的操作或者其他外部事件。使不同的计算节点能够从完全相同的初始状态运行完全相同的确定性指令序列。
	- 复制状态机的实现方法：
		- 每个节点都维护一个日志（指令序列日志），日志用于存储状态机（可以看成是CPU）将要依次执行的指令
		- 利用分布式共识协议使不同节点的指令序列日志保持一致
- 状态转移：传输的是可能是内存（上次传输后变更的内存）

## 21.Raft

### 21.1 副本的三个状态

- Leader：Leader副本接受client的更新请求，本地处理后再同步至多个其他副本；
- Follower：请求的被动更新者，从Leader接受更新请求，然后写入本地日志文件；
- Candidate：如果Follower副本在一段时间内没有收到Leader副本的心跳，则判断Leader可能已经故障，此时启动选主过程，此时副本会变成Candidate状态，直到选主结束。

### 21.2 选举

- 每一任Leader都对应一个朝代（term）编号
- 如果follower在超过规定时间内没有收到现任leader的日志更新或心跳消息，则认为leader已经失效
- follower认为现任leader失效后自己休眠一个随机时间段，在醒来后将自己转为candidate状态，并向其它所有follower请求选票
- 只有获得超过半数成员的选票的candidate才能成为新的Leader（Raft的参与者节点数目要求是奇数，如3/5/7/9）
-  新上位Leader会将更新（更大）的朝代编号通知给其它节点。任意节点收到绑定新朝代编号的消息之后将忽略所有旧朝代编号的消息
- 当一个follower收到一个candidate的拉票请求之后，两者还有互相比较日志，只有在candidate的日志超前于(或等同于）此follower的日志，该follower才会把自己的选票给它

> 先将current_term_id+1,再发送RequestVoteRPC消息

- 如果选票被瓜分，未选出leader，则current_term_id再加1

> 每个节点在一个term里只投一次票

> 日志比较的原则是，如果本地的最后一条log entry的term id更大，则更新，如果term id一样大，则日志更多的更新(index更大)。

### 21.3 日志同步

- 为每个follower维护一个nextIndex，初始化为Leader的日志尾后指针。
- Leader给follower发送AppendEntriesRPC消息，带着(pre_term_id, (nextIndex-1))之后的所有日志。
- Follower从自己的log中找是不是存在这样的log entry（是否匹配当前nextIndex），如果不存在，就给leader回复拒绝消息。
- 然后leader则将nextIndex减1，再重复，直到AppendEntriesRPC消息被接收。

### 21.4 日志复制

- 当Leader被选出来后，就可以接受客户端发来的请求了，每个请求包含一条需要被replicated state machines执行的命令。
- Leader会把它作为一个log entry append到日志中，然后给其它的server发AppendEntriesRPC请求。
- 当Leader确定一个log entry被safely replicated了（大多数副本已经将该命令写入日志当中），就apply这条log entry到状态机中然后返回结果给客户端。
- 如果某个Follower宕机了或者运行的很慢，或者网络丢包了，则会一直给这个Follower发AppendEntriesRPC直到日志一致。

### 21.5 安全性保证

- 一旦某一个节点将日志中的某条指令应用于自己的状态机之后，它确信其它节点（如果还活着）迟早会将同样的指令（在日志中相同的位置）应用到它们自己的状态机。



## 22.CAP理论

- C，指的是**强一致性**，即Linearizability，线性一致性。
- A，指的是分布式系统任何能对外访问的节点，一旦接受到了用户的请求，**就能在给定时间里返回请求结果（成功or失败），而不能返回请求阻塞或无法执行**。
- P，指的是分区容错，指的是系统在出现网络**故障**和分区的时候还能**提供服务**、并且不会破坏系统的**一致性**。

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

- CA系统：关系型数据库
- CP系统：HBase、MongoDB、Redis
- AP系统：Cassandra、CouchDB

## 23.BASE理论

> BASE是 Basically Available、Soft Sate 和 Eventual consistency 的简写

- 基本可用（Basically Available）：
	- 指一个分布式系统的一部分发生问题变得不可用时，其他部分仍然可以正常使用，也就是允许分区失败的情况出现
- 软状态（Soft State）：
	- 允许系统中的数据存在中间状态，允许系统在多个不同节点的数据副本存在更新延时
	- 硬状态（Hard State）：必须一直保持数据库状态一致性，就是指任意时刻数据必须时正确的

- 最终一致性（Eventual Consistency）：
	- 不可能一直处于软状态，在一定期限后应当保证所有副本保持数据一致性，从而达到数据的最终一致性

## 24.分区

- 分区目标：将数据和查询负载均匀分布在各个节点上，避免出现偏斜(skew)和热点(hot spot)问题。

## 25.纠删码

> 副本策略和纠删码是存储领域常见的两种数据冗余技术。相比于副本策略，纠删码具有更高的磁盘利用率。

- 基本思想是将n块原始的数据元素通过一定的计算，得到m块冗余元素（校验块）。对于这n+m块的元素，当其中任意的m块元素出错（包括原始数据和冗余数据）时，均可以通过对应的重构算法恢复出原来的n块数据。
- 生成校验的过程被成为编码（encoding），恢复丢失数据块的过程被称为解码（decoding）。
- 磁盘利用率为n/(n+m)。
- 优点：基于纠删码的方法与多副本方法相比具有冗余度低、磁盘利用率高等优点。
- 缺点：
	- 数据恢复代价高。 丢失数据块或者编码块时， RS需要读取n个数据块和校验块才能恢复数据， 数据恢复效率一定程度上制约了RS的可靠性。
		- 网络带宽的消耗，因为数据恢复需要去读其他的数据块和校验块  
		- 进行编码，解码计算需要消耗CPU资源
	- 数据更新代价高。 数据更新相当于重新编码， 代价很高， 因此常常针对只读数据，或者冷数据。



# MIT6.824

> CMU（卡内基梅隆）、MIT（麻省理工）、Stanford（斯坦福）、berkeley（加州伯克利分校）

## 一、概述

### 1.1 分布式系统的驱动力和挑战

#### 驱动力：

- 人们需要获得**更高的计算性能**。大量的计算机意味着大量的并行运算，大量CPU、大量内存、以及大量磁盘在并行的运行。
- **容错**（tolerate faults）。比如两台计算机运行完全相同的任务，其中一台发生故障，可以切换到另一台。
- 一些问题天然在空间上是分布的。例如银行转账，我们假设银行A在纽约有一台服务器，银行B在伦敦有一台服务器，这就需要一种两者之间协调的方法。所以，有一些天然的原因导致**系统是物理分布的**。
- **安全**。比如有一些代码并不被信任，但是你又需要和它进行交互，这些代码不会立即表现的恶意或者出现bug。你不会想要信任这些代码，所以你或许想要将代码分散在多处运行，这样你的代码在另一台计算机运行，我的代码在我的计算机上运行，我们通过一些特定的网络协议通信。所以，我们可能会担心安全问题，我们把系统分成多个的计算机，这样可以限制出错域。

#### 挑战：

- 因为系统中存在很多部分，这些部分又在并发执行，你会遇到**并发编程和各种复杂交互**所带来的问题，以及时间依赖的问题（比如同步，异步）。这让分布式系统变得很难。
- 另一个导致分布式系统很难的原因是，分布式系统有多个组成部分，再加上计算机**网络**，你会会遇到一些意想不到的**故障**。由多台计算机组成的分布式系统，可能会有一部分组件在工作，而另一部分组件停止运行，或者这些计算机都在正常运行，但是网络中断了或者不稳定。所以，**局部错误**也是分布式系统很难的原因。
- 人们设计分布式系统的根本原因通常是**为了获得更高的性能**，比如说一千台计算机或者一千个磁盘臂达到的性能。但是实际上一千台机器到底有多少性能是一个棘手的问题，这里有很多难点。所以通常需要倍加小心地设计才能让系统实际达到你期望的性能。

### 1.2分布式系统的抽象和实现工具

- 基础架构的类型主要是 **存储，通信（网络）和计算**。
- RPC
- 线程

### 1.3可扩展性、可用性和一致性

- 可扩展性：我们希望可以通过增加机器的方式来实现扩展，但是现实中这很难实现，需要一些架构设计来将这个可扩展性无限推进下去。
- 可用性：大型分布式系统中，错误总会发送。需要在特定的故障范围内，系统仍然能够提供服务。需要可恢复性。
	- 非易失性存储
	- 复制（replication）
- 一致性：人们常常会使用弱一致系统，你只需要更新最近的数据副本，并且只需要从最近的副本获取数据。

### 1.4 MapReduce

- Google需要一种框架，使得普通工程师也可以很容易的完成并运行大规模的分布式运算。这就是MapReduce出现的背景。

#### 1.4.1 流程

- 几个步骤，splitting、mapping、shuffling、reducing。

> 分片、格式化

- 分片操作：将源文件划分为大小相等的小数据块( Hadoop 2.x 中默认 128MB )，也就是分片( split )，Hadoop 会为每一个分片构建一个 Map 任务，并由该任务运行自定义的 map() 函数，从而处理分片里的每一条记录;
- 格式化操作：将划分好的分片( split )格式化为键值对<key,value>形式的数据，其中， key 代表偏移量， value 代表每一行内容。

> 执行MapTask

- 每个 Map 任务都有一个内存缓冲区(缓冲区大小 100MB )，输入的分片数据经过 Map 任务处理后的中间结果会写入内存缓冲区中。
- 如果写入的数据达到内存缓冲的阈值( 80MB )，会启动一个线程将内存中的溢出数据写入磁盘，同时不影响 Map 中间结果继续写入缓冲区。
- 在溢写过程中， MapReduce 框架会对 key 进行排序。
- 如果中间结果比较大，会形成多个溢写文件，最后的缓冲区数据也会全部溢写入磁盘形成一个溢写文件，如果是多个溢写文件，则最后合并所有的溢写文件为一个文件。

> 执行shuffle

- Shuffle 会将 MapTask 输出的处理结果数据分发给 ReduceTask ，并在分发的过程中，对数据按 key 进行分区和排序。

> 执行ReduceTask

- 输入 ReduceTask 的数据流是<key, {value list}>形式，用户可以自定义 reduce()方法进行逻辑处理，最终以<key, value>的形式输出。

> 写入文件

- MapReduce 框架会自动把 ReduceTask 生成的<key, value>传入 OutputFormat 的 write 方法，实现文件的写入操作。

****

#### 1.4.2 Map

- Read 阶段： MapTask 通过用户编写的 RecordReader ，从输入的 InputSplit 中解析出一个个 key / value 。

- Map 阶段：将解析出的 key / value 交给用户编写的 Map ()函数处理，并产生一系列新的 key / value 。

- Collect 阶段：数据处理完成后，一般会调用 outputCollector.collect() 输出结果，在该函数内部，它会将生成的 key / value 分片(通过调用 partitioner )，并写入一个环形内存缓冲区中(该缓冲区默认大小是 100MB )。

- Spill 阶段：即“溢写”，当缓冲区快要溢出时(默认达到缓冲区大小的 80 %)，会在本地文件系统创建一个溢出文件，将该缓冲区的数据写入这个文件。

  > 将数据写入本地磁盘前，先要对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作。
  > 写入磁盘之前，线程会根据 ReduceTask 的数量，将数据分区，**一个 Reduce 任务对应一个分区的数据**。
  > 这样做的目的是为了避免有些 Reduce 任务分配到大量数据，而有些 Reduce 任务分到很少的数据，甚至没有分到数据的尴尬局面。
  > 如果此时设置了 Combiner ，将排序后的结果进行 Combine 操作，这样做的目的是尽可能少地执行数据写入磁盘的操作。

- Combine 阶段：当所有数据处理完成以后， MapTask 会对所有临时文件进行一次合并，以确保最终只会生成一个数据文件

  > 合并的过程中会不断地进行排序和 Combine 操作，
  > 其目的有两个：一是尽量减少每次写入磁盘的数据量;二是尽量减少下一复制阶段网络传输的数据量。
  > 最后合并成了一个已分区且已排序的文件。

****

#### 1.4.3 Reduce

- Copy 阶段： Reduce 会从各个 MapTask 上远程复制一片数据（每个 MapTask 传来的数据都是有序的），并针对某一片数据，如果其大小超过一定國值，则写到磁盘上，否则直接放到内存中

- Merge 阶段：在远程复制数据的同时， ReduceTask 会启动两个后台线程，分别对内存和磁盘上的文件进行合并，以防止内存使用过多或者磁盘文件过多。

- Sort 阶段：用户编写 reduce() 方法输入数据是按 key 进行聚集的一组数据。

  > 为了将 key 相同的数据聚在一起， Hadoop 采用了基于排序的策略。
  > 由于各个 MapTask 已经实现对自己的处理结果进行了局部排序，因此， ReduceTask 只需对所有数据进行一次归并排序即可。

- Reduce 阶段：对排序后的键值对调用 reduce() 方法，键相等的键值对调用一次 reduce()方法，每次调用会产生零个或者多个键值对，最后把这些输出的键值对写入到 HDFS 中

- Write 阶段： reduce() 函数将计算结果写到 HDFS 上。

  > 合并的过程中会产生许多的中间文件(写入磁盘了)，但 MapReduce 会让写入磁盘的数据尽可能地少，并且最后一次合并的结果并没有写入磁盘，而是直接输入到 Reduce 函数。

#### 1.4.4 三次排序

- 第一次是溢写前，在内存缓冲区中完成分区后，对分区内数据按键进行快排，使分区内数据有序。
- 第二次是传输给Reduce前，磁盘内有多个分区有序的溢写文件，将溢写文件归并排序，形成分区有序的单个文件。
- 第三次是进行Reduce前，将传输给Reduce的多个文件归并排序，并送入Reduce函数。



## 二、Go编程相关/RPC



## 三、强一致性和GFS

### 3.1分布式存储系统的难点

- 分布式存储的出发点：性能
	- 分片（把数据分割到大量机器上，并行读取数据）
- 于是需要 **容错性** 来面对常态化故障
- 于是提出 **复制(replication)**
- 于是有 **一致性** 需求
- 于是就只能 降低性能

### 3.2 强一致性的直观理解

- 对于一个理想的强一致模型，你看到的就像是只有一台服务器，一份数据，并且系统一次只做一件事情。

- 你可以认为只有一台服务器，甚至这个服务器只运行单线程，它同一时间只处理来自客户端的一个请求。

- 可能会有大量的客户端并发的发送请求到服务器上。这里要求服务器从请求中挑选一个出来先执行，执行完成之后再执行下一个。

### 3.3 GFS

- 设计目标：海量数据存储、并行快速读取、全局有效。
- 分片
- 故障自动恢复（fault tolerance）
- GFS的副本保存在一个数据中心里（非设计目标）
- 单次操作数据量很大（MB），只支持顺序读取，不支持随机访问
- GFS并不保证返回正确的数据（弱一致性）
- 单个Master

> 原则与思路

- 组件失效被认为是常态事件，而不是意外事件；
- 文件非常巨大；
- 绝大部分的文件修改时采用在文件尾部追加数据；
- 应用系统与文件系统的API的协同设计提高了整个系统的灵活性，比如：放松了一致性模型的要求、引入原子性的记录追加操作等。

> Master和Chunk

- Master 是 active-standby 模式，只有一个Master在工作。
- Master节点保存了文件名和存储位置的对应关系。
- 有大量的Chunk服务器，可能会有数百个，每一个Chunk服务器上都有1-2块磁盘。
- Master服务器存储3种主要类型的元数据：文件和Chunk的命名空间【目录结构】、**文件和Chunk的对应关系**、**每个Chunk副本的存放地点**。
	- 文件名到Chunk ID或者Chunk Handle数组的对应。这个表单告诉你，文件对应了哪些Chunk。
	- 每个Chunk存储在哪些服务器上，这部分是Chunk服务器的列表。
	- 每个Chunk当前的版本号，所以Master节点必须记住每个Chunk对应的版本号。
	- 所有对于Chunk的写操作都必须在主Chunk（Primary Chunk）上顺序处理，主Chunk是Chunk的多个副本之一。所以，Master节点必须记住哪个Chunk服务器持有主Chunk。
	- 并且，主Chunk只能在特定的租约时间内担任主Chunk，所以，Master节点要记住主Chunk的租约过期时间。
- 以上数据存在内存中，部分以log的形式持久化在磁盘里（Chunk Handle的数组和Chunk版本号）。
- Master的恢复是通过 检查点、log、与chunk通信 共同完成的。
- Chunk的恢复是一主多从复制恢复的。

> 读文件

- **客户端**将 **文件名 **和 **程序指定的文件内字节偏移 发送给Master节点；**
- **Master节点 **将 **相应的Chunk标识 **和 **副本的位置 **发送给 **客户端；**
- **客户端**用文件名和Chunk索引作为**key索引**缓存副本位置信息；
  - 这里Master更新到缓存更新期间是可能读到过期数据的，缓存过期事件以及文件重新打开都会刷新缓存信息。

- **客户端**发送一个请求到 **距离自己最近的一个副本，**包含**Chunk标识和字节范围。**

> 写文件

- 客户端 向 Master节点发出 写请求（要主副本的最后一个Chunk号）；

- Master节点通过版本号 **判定/选择** 主副本；

	- Master节点会向Primary和Secondary副本对应的服务器发送消息并告诉它们，谁是Primary，谁是Secondary，Chunk的新版本是什么。
	- 这时Master和Chunk都需要把版本号写盘；

- Master节点给Primary一个租约，租约持续60s。

- 客户端会将要追加的数据通过管道发送给Primary和Secondary服务器，这些服务器会将数据写入到一个临时位置。

- 主副本服务器追加后，通知所有Secondary副本做同样的追加。

- 所有副本成功 == 返回客户端成功，否则 客户端 需要重新发起追加操作。


***

- GFS不保证Chunk的所有副本在字节级别是完全一致的，它只保证数据作为一个整体原子地被至少写入一次。

- 租约有效防止了脑裂的问题。

- 写失败和写重复会在读取的时候过滤掉。


>一致性

- GFS只保证数据作为一个整体原子地被至少写入一次
- 如果部分成功，部分失败。客户端的重新写入请求会将数据写入到多Chunk 的 同一偏移量的地方。这可能导致数据很奇怪。
- 单Master 可能 会面临单点故障、非自动恢复等问题。

> 并发写和文件块状态

- 文件块的三种一致性级别：已定义、未定义但一致、不一致。
  - 定义和未定义是说多客户端并发写同一偏移量时的覆盖顺序问题。
  - 一致不一致是说多副本同一偏移量内容是否相同。
  - 已定义：客户端写某个偏移量后，再读该偏移量的数据，读到的一定是刚才自己所写。
  - 未定义但一致：多个客户端并发写同一个偏移量，不确定谁会覆盖谁，即写完后再读，不确定是自己写的还是其他人写的。但是保证最终一致性，即并发写完成后，几个副本是一致的。
  - 不一致：即修改操作后，所有副本同一偏移量的数据并不完全相同。



## 四、VMware FT

### 4.1 复制

- 复制不能解决的问题：软件bug和硬件设计故障。
- 它能解决非关联故障。

> 两种复制方法： 状态转移和复制状态机

- 状态转移：可能是复制整个内存状态
- 复制状态机：认为内部操作是确定的，外部输入是不确定的。传输的是外部事件。

***

- 新副本的创建会使用状态转移。
- 多核交互、并发情况下，也可能要用状态转移。

> Vmware FT 的复制是机器级别的，包括内存和寄存器状态

### 4.2 VMware FT 工作原理

- 两个副本 分别运行在不同的物理机的虚拟机上。
- 客户端发送数据给 Primary副本，Primary副本 通过 log channel 将数据拷贝给 Backup副本。
- 两个副本有相同输入，相同处理模式，所以保持同步。
- 两个副本都回发响应报文，backup的报文被丢弃。
- 任一副本故障失效时，另一个副本上线（Go Alive）。

### 4.3 非确定性事件

- 不由当前内存直接决定的指令就是非确定性事件。

  - 网络数据包到来时，中断产生的位置/时间
  - 怪异指令：随机数生成器、获取当前时间的指令和计算机唯一ID
  - 多CPU并发


> Log Channel 传输的日志内容

- 事件发生时的指令序号（为了同步中断和输入）
- 日志条目类型（普通网络输入或怪异指令）
- 数据（普通网络输入），Primary执行结果（怪异指令）

> Bounce Buffer机制

- 物理服务器的网卡会将网络数据包拷贝给VMM的内存，之后，网卡中断会发送给VMM，并说，一个网络数据包送达了。
- 这时，VMM会暂停Primary虚机，记住当前的指令序号，将整个网络数据包拷贝给Primary虚机的内存，之后模拟一个网卡中断发送给Primary虚机。
- 同时，将网络数据包和指令序号发送给Backup。
- Backup虚机的VMM也会在对应的指令序号暂停Backup虚机，将网络数据包拷贝给Backup虚机，之后在相同的指令序号位置模拟一个网卡中断发送给Backup虚机。

### 4.4 输出控制

- 在Backup虚拟机确认收到了相应Log条目之前，Primary不会产生输出（响应）。
- Primary等Backup的回复可能会影响性能，至少是高延时。
- 有时候需要Primary停下来等Backup，使Backup不会落后太多。这非常损伤性能。

### 4.5 重复输出

- Primary在响应客户端后挂了。
- Backup的Log堆积了很多。
- Backup上线，处理到上任Primary的最后一个请求时，再次响应。

> TCP Channel意外的解决了这个问题

- 当Backup接管服务时，因为它的状态与Primary相同，所以它知道TCP连接的状态和TCP传输的序列号。
- 当Backup生成回复报文时，这个报文的TCP序列号与之前Primary生成报文的TCP序列号是一样的。
- 客户端的TCP栈会发现这是一个重复的报文，它会在TCP层面丢弃这个重复的报文，用户层的软件永远也看不到这里的重复。

### 4.6 Test-and-Set服务

- **fail-stop故障**： **当服务进程 崩溃故障** ，且能够被别的正常服务进程检测到它的故障，则称它进入到 失败停止故障 。

- **Split-Brain**（脑裂）：在多机热备的高可用 (HA) 系统中，当两个节点心跳突然断开，就分裂为了两个独立的个体，由于互相失去联系，都认为对方出现了故障，因此都会去争抢对方的资源，争抢启动，由此就会发生严重的后果。
  - 网络问题 — 节点间网络异常造成集群发生物理分离，造成脑裂问题
  - 节点负载 — 如果 master 节点负载过高，则可能造成 master 节点停止响应，从而脱离集群，集群重新选主，恢复响应后出现脑裂问题

> 脑裂问题的解决方案

- 引入第三方权威机构求证，来决定哪个备份上线	— — Test and set服务
- majority、投票、quorum机制

> Test-and-Set

- 这个服务会在内存中保留一些标志位，当你向它发送一个Test-and-Set请求，它会设置标志位，并且返回旧的值。
- Primary和Backup都需要获取Test-and-Set标志位，这有点像一个锁。为了能够上线，它们或许会同时发送一个Test-and-Set请求，给Test-and-Set服务。
- 当第一个请求送达时，Test-and-Set服务会说，这个标志位之前是0，现在是1。
- 第二个请求送达时，Test-and-Set服务会说，标志位已经是1了，你不允许成为Primary。



## 六、Raft

### 6.1 脑裂

- MapReduce、GFS和VMware FT的共性：它们需要一个单节点来决定，在多个副本中，谁是主。
- 使用一个单节点的好处是，它不可能否认自己，它的决策就是整体的决策。缺点是，它本身又是一个单点故障。
- 所以需要决策者是多副本的。

<img src="https://pic3.zhimg.com/v2-f9ef67ca688e619c6ea9c3d3ca728c62_r.jpg" alt="preview" style="zoom:67%;" />

- 很长一段时间以来，上图这种情况的两种解决方案（必须要两个节点同时响应）：
  - 昂贵的不可能故障网络
  - 有延时的人工修复网络

### 6.2 过半票决（Majority Vote)

- **网络分区**：当网络出现故障，将网络分割成两半，网络的两边独自运行，且不能访问对方。

> 过半票决

- 首先你要有奇数个服务器。
- 然后为了完成任何操作（Raft的Leader选举、提交一个Log条目），**在任何时候为了完成任何操作，你必须凑够过半的服务器来批准相应的操作**。

> 特性

- 任意两组过半服务器，至少有一个服务器是重叠的。
  - 因此，新的Leader必然知道旧Leader使用的任期号（term number)

> 过半票决系统

- 两个系统中的一个叫做Paxos，Raft论文对这个系统做了很多的讨论；
- 另一个叫做 ViewStamped Replication（VSR）。尽管Paxos的知名度高得多，Raft从设计上来说，与VSR更接近。

### 6.3 Raft（再来一次！！！！！！！）

- 客户端将PUT请求的发送到Leader节点
- Raft将该操作提交到多副本的日志中
- Raft节点相互交互，直到过半的节点新增了该日志
- 当Leader节点知道过半节点有了该操作的拷贝后，Leader节点就能真的执行这个操作了。
- 然后发送响应给客户端
- 同时，Leader将请求被commit的消息（夹带在下一次AppendEntries RPC中）发送给其他副本。

### 6.4 Raft Log（日志）

- 对于这些复制状态机来说，所有副本不仅要执行相同的操作，还需要用相同的顺序执行这些操作。
- Log与其他很多事物，共同构成了Leader**对接收到的客户端操作分配顺序**的机制。

- Log的另一个用途是Follower副本收到了操作，但是还没有执行操作时。该**副本需要将这个操作存放在Log处**，直到收到了Leader发送的新的commit号才执行。
- **Leader需要在它的Log中记录操作**，因为这些操作可能需要重传给Follower。
- 持久化存储Log **帮助重启的服务器恢复状态**。

### 6.5 应用层接口

> 俩接口

- 第一个接口是key-value层用来转发客户端请求的接口。如果客户端发送一个请求给key-value层，key-value层会将这个请求转发给Raft层，并说：请将这个请求存放在Log中的某处。
- 这个接口实际上是个函数调用，称之为Start函数。这个函数只接收一个参数，就是客户端请求。key-value层说：我接到了这个请求，请把它存在Log中，并在committed之后告诉我。

- 另一个接口是，随着时间的推移，Raft层会通知key-value层：哈，你刚刚在Start函数中传给我的请求已经commit了。

### 6.6 Leader选举（Election）

- Paxos就没有Leader，可以通过一组服务器来共同认可Log的顺序。
- Raft使用term number来区分不同的Leader，一个term最多有一个Leader。
- 每个Raft节点都有一个选举定时器（Election Timer），如果在这个定时器时间耗尽之前，当前节点没有收到任何当前Leader的消息，这个节点会认为Leader已经下线，并开始一次选举。
- 开始一次选举的意思是，当前服务器会增加任期号（term number）。
- 之后，当前服务器会发出请求投票（RequestVote）RPC，这个消息会发给所有的其他Raft节点。默认投自己一票。
- 任意一个任期内，每一个节点只会对一个候选人投一次票。当选需要过半服务器的认可投票。
- 赢得选举的服务器需要立刻发送一条AppendEntries消息给其他所有的服务器，其他服务器通过接收特定任期号的AppendEntries来知道，选举成功了。

> 如果网络延迟或者故障 导致Leader发出的心跳丢了几个，就可能发生网络分区-脑裂问题

- 少数分区的Leader凑不齐过半服务器，所以它永远不会commit客户端请求，也不会执行这个请求并响应客户端。

### 6.7 选举计时器(Election Timer)

- Split-Vote（分割选票）：选举失败可能的场景，即候选人几乎同时参加竞选，它们分割了选票。
- 为了避免上述问题，选举计时器会随机选择超时时间。
- 最小超时时间应该大于一个心跳间隔，是几倍。
- 最大超时时间可能决定了系统恢复时间。
- 不同节点的超时时差应该至少大于一条PRC的往返时间(Round-Trip)，这就要求最大超时时间和最小超时时间间的间隔足够大，随机出来好的可能也就大。
- 这个超时时间应该每次都重新随机。



## 七、Raft补充

### 7.1 日志恢复

- 新Leader上任后需要认识到：
  - 过半节点有的Log可能已commit，不能丢弃。
  - 节点们应该有完全一样的请求。

### 7.2 选举约束

> 投票检查

- 候选人最后一条Log条目的任期号**大于**本地最后一条Log条目的任期号；

- 或者，候选人最后一条Log条目的任期号**等于**本地最后一条Log条目的任期号，且候选人的Log记录长度**大于等于**本地Log记录的长度

### 7.3 快速恢复（接7.1）

- 让Follower返回足够的信息给Leader，这样Leader可以以任期（Term）为单位来回退，而不用每次只回退一条Log条目。
- 在恢复Follower的Log时，如果Leader和Follower的Log不匹配，Leader只需要对每个不同的任期发送一条AppendEntries，而不用对每个不同的Log条目发送一条AppendEntries。

> 携带三个额外信息

- XTerm：这个是Follower中与Leader冲突的Log对应的任期号。在之前（7.1）有介绍Leader会在prevLogTerm中带上本地Log记录中，前一条Log的任期号。如果Follower在对应位置的任期号不匹配，它会拒绝Leader的AppendEntries消息，并将自己的任期号放在XTerm中。如果Follower在对应位置没有Log，那么这里会返回 -1。
- XIndex：这个是Follower中，对应任期号为XTerm的第一条Log条目的槽位号。
- XLen：如果Follower在对应位置没有Log，那么XTerm会返回-1，XLen表示空白的Log槽位数。

### 7.4 持久化

- 在raft论文里，有且仅有三个数据是需要持久化存储的。它们分别是Log、currentTerm、votedFor。
  - 当服务器重启时，唯一能用来重建应用程序状态的信息就是存储在Log中的一系列操作
  - currentTerm和votedFor都是用来确保每个任期只有最多一个Leader。

> synchronous disk updates

- 因为log需要持久化，所以同步磁盘更新的代价可能会非常高。
- 用SSD或电池供电的DRAM做持久化，能解决上述问题

- 无论是Leader还算Follower 都需要先将Log持久化，再发送RPC或响应。

> 重启后读取log恢复应用程序状态这一行为，很慢，需要解决

### 7.5 日志快照

- 当Raft认为它的Log过于庞大时，会要求应用程序在Log的特定位置，对其状态做一个快照。
  - 快照的实质就是一个key-value表单，它会对应于一个 Log槽位号。
- 之后，快照被持久化到磁盘，该槽位号之前的Log被丢弃。

> Log丢弃可能带来的问题，如果Follower落后Leader太多，会导致F重启后，L没有F需要的快照了

- 解决方法是同步Leader的快照给Follower。

### 7.6 线性一致性(Linearizability)

- 线性一致性(Linearizability)：又叫强一致性、原子一致性。
- 一个服务是线性一致的，那它表现的就像只有一个服务器，每次执行一个客户端请求。

> 正式一点的

- 一个系统的执行历史是一系列的客户端请求，或许这是来自多个客户端的多个请求。如果执行历史整体可以按照一个顺序排列，且**排列顺序与客户端请求的实际时间相符合**，那么它是线性一致的。
- 并且，每一个读操作都看到的是最近一次写入的值。

> 线性一致顺序

- 要达到线性一致，需要为操作生成一个线性一致的顺序。对于这个顺序，有两个限制条件：
  - 如果一个操作在另一个操作开始前就结束了，那么这个操作必须在执行历史中出现在另一个操作前面。
  - 执行历史中，读操作，必须在相应的key的写操作之后（就是说，如果先写1，再写2，那么写完2，才能读到2。）

- 如果有个线性一致顺序符合上述两个条件，则该执行历史是线性一致的。


> 如何判断线性一致性还是顺序一致性

- 将多用户的执行历史整合成一条线性操作序列：
  - 如果可以产生一个方案，它既不矛盾也符合客户实际请求时间，则线性一致。
  - 如果可以产生一个方案，它不矛盾但不符合客户实际请求时间，则顺序一致。



## 八、Zookeeper

### 8.1 线性一致性（1）

- 线性一致性的证明方法就是在执行历史中找到环的存在，使它不是线性的。

> 对重复请求服务的合理性

- 服务器处理重复请求的合理方式是，服务器会根据请求的唯一号或者其他的客户端信息来保存一个表。这样服务器可以记住，哦，我之前看过这个请求，并且执行过它，我会发送一个相同的回复给它，因为我不想执行相同的请求两次。
- 但这可能导致响应结果已经过时了。

<img src="https://pic1.zhimg.com/80/v2-fb788e7cfeebfe17c08c5a917f313d34_720w.jpg" alt="img" style="zoom:50%;" />

- 这种情况返回3和4可能都是合法的，这取决于设计者。

> 线性一致性可以是为了 在某种情况发生时，有一个准则来解释该现象是否合理。

### 8.2 Zookeeper

- 一种通用的协调服务（不同于Raft库，需要设计应用程序与Raft的交互，Zookeeper提供了独立的，通用的系统）
- Zookeeper的Leader有两层，上层是与客户端交互的Zookeeper，下层是类似于Raft的Zab协议(Zookeeper Atomic Broadcast)。
- 同Raft一样，Leader也会面临单点故障。

> 因为完全没办法保证除Leader外的其他节点数据是最新的，所以把读请求发送给Follower是不满足线性一致性

- 这里Zookeeper放弃了线性一致性。。。提供的应该是顺序一致性（请求先来先服务，调度不改变代码顺序）

### 8.3 zk的一致保证(Consistency Guarantees)

- 写请求的线性一致性。
  - 客户端并发的发送写请求，Zookeeper保证线性执行，且符合写请求的实际时间。
- FIFO客户端序列：任何一个客户端的请求，都会按照客户端指定的顺序来执行。
  - 对单个客户端，读写请求是线性一致的。

> 具体而言

- 对于同一客户端发起的多个写请求，zookeeper给它们打标号，并按标号顺序执行。
- 对于同一客户端发起的多个读请求，保证后一个读请求读取的Log位置在前一个读的Log位置之后。
  - 即第二个读请求至少看到第一个读请求的状态。
  - 实现方法是，为每个Log日志打上zxid标签，读请求的回复会带上zxid

### 8.4 同步操作(Sync)

- 如果想要看到某个数据的最新状态，可以先发一个sync请求，再发读请求。

### 8.5 就绪文件(Ready File/znode)

- *这里的file对应的就是论文里的znode，Zookeeper以文件目录的形式管理数据，所以每一个数据点也可以认为是一个file*

> Zookeeper如何保证多个znode里file的原子性更新?

- 首先是当 Ready File 存在时才能读，否则Ready File就是在更新。
- 所以，在更新前先删掉所有的Ready File，更新完成后在重建。
- 因为写请求是线性一致的，所以当看到Create 之后的读请求，就能看到所有之前的写操作了。

> 如果读 ready file 中间 出现 更新操作呢？

- 检查存在(exist)命令会对 ready file 建立一个watch。
- Zookeeper可以保证如果某个人删除了Ready file，相应的通知，会在任何后续的读请求之前，发送到客户端。客户端会先收到有关Ready file删除的通知，之后才收到其他在Log中位于删除Ready file之后的读请求的响应。



## 九、Zookeeper作用

### 9.1 Zookeeper API

> 什么是etcd？

- etcd 是一个可靠 key-value 存储的分布式系统。当然，它不仅仅用于存储，还提供共享配置及服务发现。

- 一致性协议： etcd 使用 Raft 协议。Zookeeper使用 ZAB（类Paxos协议）；
- 数据存储：etcd 多版本并发控制（MVCC）数据模型 ， 支持查询先前版本的键值对；
- 项目活跃度：etcd 社区与开发活跃，Zookeeper 感觉已经快死了；
- API：etcd 提供 HTTP+JSON, gRPC 接口，跨平台跨语言，Zookeeper 需要使用其客户端；
- 访问安全方面：etcd 支持 HTTPS 访问，Zookeeper 在这方面缺失；

> Zookeeper的作用

- Zookeeper API可以用来实现容错的 VMware FT 的 Test-and-Set 服务。
- 可以用它来发布其他服务器使用的配置信息。
- 选举Master、保存Master的状态。
- MapReduce中，worker结点在Zookeeper中写入一个文件来注册自己，master结点将工作写入zk，由worker结点逐个写出。
- 说通俗一点就是 保存数据中心的各应用的关键状态。

> Zookeeper API

- 像是一个文件系统。它有一个层级化的目录结构，有一个根目录（root），之后每个应用程序有自己的子目录。

- 这么设计的一个原因：Zookeeper被设计成要被许多可能完全不相关的服务共享使用。
- 实际上不是文件系统：不能mount、ls、cat，只是层次化命名了，能 "/APP2/X" 这样RPC访问。
- 所有文件和目录都叫znodes，3种类型的znode。
  - 第一种Regular znodes。一旦创建，就永久存在，除非你删除了它。
  - 第二种是Ephemeral znodes（临时结点）。如果Zookeeper认为创建它的客户端挂了，它会删除这种类型的znodes。这种类型的znodes与客户端会话绑定在一起，所以客户端需要时不时的发送心跳给Zookeeper，告诉Zookeeper自己还活着，这样Zookeeper才不会删除客户端对应的ephemeral znodes。
  - 最后一种类型是Sequential znodes。它的意思是，当你想要以特定的名字创建一个文件，Zookeeper实际上创建的文件名是你指定的文件名再加上一个数字。当有多个客户端同时创建Sequential文件时，Zookeeper会确保这里的数字不重合，同时也会确保这里的数字总是递增的。
- Zookeeper以RPC的方式暴露以下API：
  - `CREATE(PATH，DATA，FLAG)`。入参分别是文件的全路径名PATH，数据DATA，和表明znode类型的FLAG。CREATE的语义是排他的。
  - `DELETE(PATH，VERSION)`。入参分别是文件的全路径名PATH，和版本号VERSION。当且仅当znode的当前版本号与传入的version相同，才执行操作。
  - `EXIST(PATH，WATCH)`。入参分别是文件的全路径名PATH，和一个有趣的额外参数WATCH。通过指定watch，你可以监听对应文件的变化。判断文件是否存在和watch文件的变化，在Zookeeper内是原子操作。
  - `GETDATA(PATH，WATCH)`。入参分别是文件的全路径名PATH，和WATCH标志位。这里的watch监听的是文件的内容的变化。
  - `SETDATA(PATH，DATA，VERSION)`。入参分别是文件的全路径名PATH，数据DATA，和版本号VERSION。如果你传入了version，那么Zookeeper当且仅当文件的版本号与传入的version一致时，才会更新文件。
  - `LIST(PATH)`。入参是目录的路径名，返回的是路径下的所有文件。

### 9.2 zk实现计数器

- 为了适应高并发场景下的字段自增，需要加入version参数。version匹配才能更新。

~~~go
WHILE TRUE:
    X, V = GETDATA("F")
    IF SETDATA("F", X + 1, V):
        BREAK
~~~

- 上述内容被称为 mini-transaction，即对计数器达成了**读-更改-写**的原子操作。

### 9.3 zk实现非扩展锁(Non-Scalable Lock)

> 获得锁

~~~go
WHILE TRUE:
    IF CREATE("f", data, ephemeral=TRUE): RETURN
    IF EXIST("f", watch=TRUE):
        WAIT
~~~

- 羊群效应（Herd Effect）：所有的客户端都尝试对一个临时节点去加锁，当一个锁被占有的时候，其他的客户端都会监听这个临时节点。一旦锁被释放，Zookeeper反向通知添加监听的客户端，然后大量的客户端都尝试去对同一个临时节点创建锁，最后也只有一个客户端能获得锁，但是大量的请求造成了很大的网络开销，加重了网络的负载，影响Zookeeper的性能。

- 解决方案：
  - 所有客户端都尝试去创建临时有序节点以获取锁
  - 序号最小的临时有序节点获得锁
  - 未获取到锁的客户端给自己的上一个临时有序节点添加监听
  - 获得锁的客户端进行自己的操作，操作完成之后删除自己的临时有序节点
  - 当监听到自己的上一个临时有序节点释放了锁，尝试自己去加锁
  - 操作完成之后释放锁
  - 之后剩下的客户端重复加锁和解锁的操作

### 9.4 zk实现可扩展锁(Scalable Lock)

~~~go
CREATE("f", data, sequential=TRUE, ephemeral=TRUE)
WHILE TRUE:
    LIST("f*")
    IF NO LOWER #FILE: RETURN
    IF EXIST(NEXT LOWER #FILE, watch=TRUE):
        WAIT
~~~

- 这一节没细看，待三刷。。。

### 9.5 链复制（chain replication）

- CRAQ（Chain Replication with Apportioned（分摊） Queries）：通过复制实现容错，以链复制API请求的方式，提供了与Raft不一样的属性。
  - 读请求分发到任意副本去执行，还可以保证线性一致性（Linearizability）。

> Chain Replication

- 在Chain Replication中，有一些服务器按照链排列。第一个服务器称为HEAD，最后一个被称为TAIL。

- 写请求

  - 写请求总是发送给HEAD。

  - 当HEAD收到了写请求，将本地数据更新，之后会再将写请求通过链向下一个服务器传递。

  - 下一个服务器执行完写请求之后，再将写请求向下一个服务器传递，以此类推，所有的服务器都可以看到写请求。

  - 当写请求到达TAIL时，TAIL将回复发送给客户端，表明写请求已经完成了。

- 读请求
  - 对于读请求，如果一个客户端想要读数据，它将读请求发往TAIL。
  - TAIL直接根据自己的当前状态来回复读请求。
- 是线性一致性的，TAIL能看见所有的读写请求，状态就像是只有TAIL一样。
- 故障恢复(Fail Recover) ：如果故障了，一定是停在链中的某个服务器上，该服务器之前都看到了写请求，之后都没看到。
  - HEAD出现故障，那HEAD的下一个结点接手成为HEAD。TAIL出现故障，那TAIL的上一结点接手成为TAIL。
  - 中间结点故障，需要做的就是将故障节点从链中移除。故障节点的前一个节点或许需要重发最近的一些写请求给它的新后继节点。
- Chain Replication并不能抵御网络分区，也不能抵御脑裂。
  - 比如，HEAD和它的后继结点间网络故障，则将有两个HEAD。
- 缺点：如果有一个副本很慢，那整个服务状态就被拖垮了。

### 9.6 链复制的配置管理器

- 总是会有一个外部的权威（External Authority）来决定谁是活的，谁挂了，并确保所有参与者都认可由哪些节点组成一条链，这样在链的组成上就不会有分歧。这个外部的权威通常称为Configuration Manager。

- Configuration Manager的工作就是监测节点存活性，一旦Configuration Manager认为一个节点挂了，它会生成并送出一个新的配置，它描述了链的新的定义，包含了链中所有的节点，HEAD和TAIL。
- Configuration Manager通常会基于Raft或者Paxos，保证自身是容错的。在CRAQ的场景下，它会基于Zookeeper。

> 题外话： Spanner 就使用了 Paxos 来保证一致性



## 十、 Aurora

### 10.1 Aurora（极光）

- Amazon的 高性能， 高可靠数据库。云基础设施。
- 基于Mysql 构建的。

> DB如何实现一个故障可恢复事务（Crash Recoverable Transaction）。

- 事务是指将多个操作打包成原子操作，并确保多个操作顺序执行。如果在执行的任何时候出现故障，我们需要确保故障恢复之后，要么所有操作都已经执行完成，要么一个操作也没有执行。
- 通常来说，事务是通过对涉及到的每一份数据加锁来实现。
- 在事务提交之前，数据的修改还只在本地的缓存中，并没有写入到硬盘。为了让数据库在故障恢复之后，还能够提供同样的数据，在允许数据库软件修改硬盘中真实的data page之前，数据库软件需要先在WAL中添加Log条目来描述事务。所以在提交事务之前，数据库需要先在WAL中写入完整的Log条目，来描述所有有关数据库的修改，并且这些Log是写入磁盘的。
- 对于Aurora来说，实际上也使用了undo/redo日志，用来撤回未完成事务的操作。

> Amazon RDS（Relational Database Service）

- RDS是第一次尝试将数据库在多个AZ之间做复制，这样就算整个数据中心挂了，你还是可以从另一个AZ重新获得数据而不丢失任何写操作。
- RDS有且仅有一个EC2实例作为数据库。这个数据库将它的data page和WAL Log存储在EBS，而不是对应服务器的本地硬盘。当数据库执行了写Log或者写page操作时，这些写请求实际上通过网络发送到了两个EBS服务器。所有这些服务器都在一个AZ中。
- 每一次数据库软件执行一个 **写操作** ，Amazon会自动的，对数据库无感知的，将写操作拷贝发送到另一个数据中心的AZ中。

- AZ2的副数据库执行和AZ1相同的操作，成功后将回复返回给AZ1的主数据库，主数据库看到这个回复之后，才会认为写操作完成了。

> Aurora

- 架构表述一：在替代EBS的位置，有6个数据的副本，位于3个AZ，每个AZ有2个副本。每个写请求都需要以某种方式发送给这6个副本。

- 架构表述二：通过网络传递的数据只有 **Log条目**。
  - 比起RDS发送data page（多个8kb)，log就小的多。
  - 因为RDS只需要支持读写数据块，而Aurora能理解数据库的Log，所以RDS是通用存储，而Aurora是应用(MySQL)定制的。
- 架构表述三：Quorum~（法定人数）~形成后，写操作就成功了。不用等6个，4个就行。

> Aurora的容错目标

- 单AZ故障，不影响写。
- 两个AZ故障，不影响读。
- 允许出现短暂的慢副本。
- 新副本的快速生成(Fast Re-replication)

### 10.2 Quorum复制机制

> 经典Quorum配置：假设有N个副本。为了能够执行写请求，必须要确保写操作被W个副本确认，W小于N。所以你需要将写请求发送到这W个副本。如果要执行读请求，那么至少需要从R个副本得到所读取的信息。这里的W对应的数字称为Write Quorum，R对应的数字称为Read Quorum。

- Quorum系统要求，任意你要发送写请求的W个服务器，必须与任意接收读请求的R个服务器 **有重叠**。
  - 即：R加上W必须大于N。
- 所以任何读请求可以从至少一个看见了之前写请求的服务器得到回复。
- 每次写请求都会增加数据的 **版本**，读请求使用最高版本号。
- 如果不能与Quorum数量的服务器通信（不管是Read Quorum还是Write Quorum），那么只能不停的重试。

> 实际工作场景

- 执行写请求时，将新的数值和对应的版本号给所有N个服务器，但是只会等待W个服务器确认。
- 对于读请求，将读请求发送给所有的服务器，但是只等待R个服务器返回结果。
- 通过设置 read quorum 和 write quorum 的数量，可以调整服务器的读取性能。

> Aurora的 Quorum设置

- Aurora的Quorum系统中，N=6，W=4，R=3。
- W等于4意味着，当一个AZ彻底下线时，剩下2个AZ中的4个服务器仍然能完成写请求。
- R等于3意味着，当一个AZ和一个其他AZ的服务器下线时，剩下的3个服务器仍然可以完成读请求。
- 当3个服务器下线了，系统仍然支持读请求，但是却不能支持写请求。
- 当3个服务器挂了，现在的Quorum系统有足够的服务器支持读请求，并据此重建更多的副本。

### 10.3 Aurora读写存储服务器

- Aurora的Quorum用来在副本存储服务器上追加Log记录。存储服务器内存最终存储的还是数据库服务器磁盘中的page。
- 存储服务器的内存中，会有自身磁盘中page的cache。
- 当新的Log到来时，不必立即执行这个更新，可以等到数据库服务器或者恢复软件想要查看那个page时才执行。
- 存储服务器会在内存中缓存一个旧版本的page和一系列来自于数据库服务器有关修改这个page的Log条目。（page+log list）
- 一个新的事务读取这个page时，它会发出一个读请求。请求发送到存储服务器，会要求存储服务器返回当前最新的page数据。
- 在这个时候，存储服务器才会将Log条目中的新数据更新到page，并将page写入到自己的磁盘中，之后再将更新了的page返回给数据库服务器。
- 同时，存储服务器在自身cache中会删除page对应的Log列表，并更新cache中的page
- 数据库服务器会记录每个存储服务器收到的最高连续的Log条目号。
- 当一个数据库服务器需要执行读操作，它只会挑选拥有最新Log的存储服务器，然后只向那个服务器发送读取page的请求。
- 数据库服务器执行了Quorum Write，没有执行Quorum Read。因为它知道哪些存储服务器有最新的数据，然后可以直接从其中一个读取数据

> 什么时候需要 Quorum Read

- 当某个EC2上的**数据库服务器崩溃时**，新建的EC2数据库会执行Quorum Read,来获取所有Log条目。
- 如果某个Log丢失，则告诉存储服务器丢掉该条目及其之后的Log条目。
- 因为只有当一个事务的所有Log条目存在于Write Quorum时，这个事务才会被commit，所以对于已经commit的事务我们肯定可以看到相应的Log。这里我们**只会丢弃未commit事务对应的Log条目**。

### 10.4 数据分片(Protection Group)

- 每个PG(Protection Group)都是一组6个副本的Aurora。
- 每个Protection Group存储了所有data page的一个子集，以及这些data page相关的Log条目。
- 如果一个存储服务器崩了，那可能同时也带走了其他数百个数据库的数据。也需要恢复数百个数据库的存储，这是一个很长的时间。
- 策略是把该存储服务器的数据库分到数百个存储服务器上去恢复，每个存储服务器就恢复一个，这样就快多了（有钱任性）。
- 这意味着，如果一个服务器挂了，它可以并行的，快速的在数百台服务器上恢复。

### 10.5 只读数据库（Read-only Database）

- 只有一个存储服务器负责写入服务，可以简易的对Log进行编号。
- 除了主数据库用来处理写请求，同时也有一组只读数据库。可以支持最多15个只读数据库，用来分担读请求。
- 只读数据库会落后主数据库一点，但是对于大部分的只读请求来说，这没问题。
- 在主数据库发给只读数据库的Log流中，主数据库需要指出，哪些事务commit了，只读数据库需要等到事务commit了再应用对应的Log。

> 微事务（Mini-Transaction）和VDL

- 数据库服务器可以通知存储服务器说，这部分复杂的Log序列只能以原子性向只读数据库展示，也就是要么全展示，要么不展示。

> VCL

- 。。。都待补充



### 10.6 总结

- 最后一件有意思的事情是，论文中的一些有关云基础架构中什么更重要的隐含信息。例如：
  - 需要担心整个AZ会出现故障；
  - 需要担心短暂的慢副本，这是经常会出现的问题；
  - 网络是主要的瓶颈，毕竟Aurora通过网络发送的是极短的数据，但是相应的，存储服务器需要做更多的工作（应用Log），因为有6个副本，所以有6个CPU在复制执行这些redo Log条目，明显，从Amazon看来，网络容量相比CPU要重要的多。




## 十一、Frangipani 素馨花

### 11.1 Frangipani的背景

- 关注点：缓存一致性，分布式事务和分布式故障恢复
- **缓存一致性(cache coherence)**：我缓存了一些数据，之后你修改了实际数据但是并没有考虑我缓存中的数据，必须有一些额外的工作的存在，这样我的缓存才能与实际数据保持一致。

- Frangipani是一个网络文件系统，运行在多个工作站(OS+外设)。
- 文件系统的数据结构（如文件内容、inode、目录、目录的文件列表、inode和块的空闲状态），都存在一个叫做Petal的共享虚拟磁盘服务中。

- Petal存在远程服务器上，且是容错的（有副本）。
- 当Frangipani~(ˌfrændʒi'pæni)~需要读写文件时，它会向正确的Petal服务器发送RPC，并说，我需要这个块，请读取这个块，并将数据返回给我。

> 功能

- Frangipani被期望用来实现共享文件。
- 用户读自己home下的文件居多，读共享文件较少，所以期望可以把文件缓存在本地。
- Frangipani还支持Write-Back缓存

- 文件系统的逻辑需要存在于每个工作站上。Petal不知道文件系统，文件，目录，它只是一个很直观简单的存储系统，所有的复杂的逻辑都在工作站中的Frangipani模块中。

### 11.2 Frangipani的挑战

- 缓存
  - 工作站上的一些修改需要被其他工作站看到，即要**缓存（强）一致性**
  - 如果我缓存了一个数据，并且其他人在他的缓存中修改了这个数据，那么我的缓存需要自动的应用那个修改！！！
- 去中心化架构带来的大量逻辑存在于客户端之中带来的问题
  - **故障恢复**：需要单个服务器的故障恢复，我希望我的工作站的崩溃不会影响其他使用同一个共享系统的工作站。
  - **原子性(Atomicity)**：对在同一个时间修改同一个目录的操作，希望看到每个操作就像在一个时间点发生，而不是一个时间段发生，即时生效。

### 11.3 Frangipani的锁服务

- Frangipani的缓存一致性核心是由锁保证的。
- 在原子性和故障恢复中将会再次看到锁。
- 除了Frangipani服务器（也就是工作站），Petal存储服务器，在Frangipani系统中还有第三类服务器，锁服务器（假设只有一个）。
- 在锁服务器里面，有一个表单，就叫做locks。我们假设每一个锁以文件名来命名，所以对于每一个文件，我们都有一个锁，而这个锁，可能会被某个工作站所持有。

<img src="https://pic1.zhimg.com/80/v2-e209005233866e86ebdc53e7536a77e4_720w.jpg" alt="img" style="zoom:50%;" />

- 在每个工作站中，Frangipani模块也会有一个lock表单，表单会记录文件名(inode)、对应的锁的状态和文件的缓存内容。这里的文件内容可能是大量的数据块，也可能是目录的列表。

> 机制

- 当一个Frangipani服务器决定要读取文件（比如读取目录 /、读取文件A、查看一个inode）首先，它会向一个锁服务器请求文件对应的锁，之后才会向Petal服务器请求文件或者目录的数据。（先请求锁，后请求对应的数据）
  - 基本上来说，不允许在没有锁保护的前提下缓存数据
- 每一个工作站的锁至少有两种模式。
  - Busy状态：工作站可以读或者写相应的文件或者目录的最新数据，可以在创建，删除，重命名文件的过程中。
  - Idle状态：在工作站完成了一些操作之后，比如创建文件，或者读取文件，它会随着相应的系统调用（例如rename，write，create，read）释放锁。系统调用后，这个锁仍然被这个工作站持有，但是工作站并不再使用它。
    - 延迟释放的原因是，创建好的文件一般都要接着用。
- 如果在释放锁之前，修改了锁保护的数据，就必须将修改了的数据写回到Petal，只有在Petal确认收到了数据，你才可以释放锁，也就是将锁归还给锁服务器。（先写回数据，再释放锁）
- 最后再从工作站的lock表单中删除关文件的锁的记录和缓存的数据。

> 缓存一致性

- 工作站和锁服务器之间的缓存一致协议协议包含了4种不同的消息。本质上是一些单向的网络消息。
- **Request **消息：从工作站发给锁服务器。请求锁。
  - 如果从锁服务器的lock表单中发现锁已经被其他人持有了，那锁服务器不能立即交出锁。
- 一旦锁被释放了，锁服务器会回复一个 **Grant** 消息给工作站, 工作站就拥有锁了。
  - 这里的Request和Grant（授予）是异步的。

- 锁服务器收到加锁请求，会从lock表单中查找锁持有者，并发送一个 **Revoke**（撤销）消息给当前持有锁的工作站。请求释放锁。
- 工作站收到Revoke请求后，如果锁的状态是Idle，首先需要将修改了的缓存数据发回给Petal，然后向锁服务器发送一条 **Release** 消息。

> 优化

- 如果有大量的工作站需要读取文件，但是没有人会修改这个文件，它们都可以同时持有对这个文件的读锁。
- 如果某个工作站需要修改这个已经被大量工作站缓存的文件时，那么它首先需要Revoke所有工作站的读锁，然后再修改文件。

> 保护机制

- 工作站每隔30秒会将所有修改了的缓存写回到Petal中。

### 11.4 原子性

- 比如，创建一个文件，会涉及到标识一个新的inode、初始化一个inode（inode是用来描述文件的一小份数据）、为文件分配空间、在目录中为新文件增加一个新的名字，这里有很多步骤，很多数据都需要更新。
- 我们希望其他的工作站 **要么发现文件不存在，要么文件完全存在**，但是我们绝不希望它看到中间状态。
- Frangipani在内部实现了一个以锁为核心的分布式事务系统，来满足这一要求。
- 首先获取所有需要读写数据的锁，在完成操作并将修改了的数据写回Petal之前，我的工作站不会释放任何一个锁。

### 11.5 Frangipani Log（故障恢复）

- 场景：Frangipani在进行一个写事务，获取了大量锁，部分数据写回Petal，部分未写回时崩溃。
- 需要释放锁：让其他工作站能访问该文件
- 不能释放锁：可能读到错误数据
- Frangipani与其他的系统一样，需要通过预写式日志(Write-Ahead Log)实现故障可恢复的事务(Crash Recoverable Transaction)。
- Frangipani对于每个工作站都保存了一份独立的Log，且存储在Petal中。
- 每个工作站按照12345为自己的Log编号，以便Frangipani检测Log的结尾。
- 每个Log条目还有一个用来描述一个特定操作中所涉及到的所有数据修改的数组。数组中的每一个元素会有一个Petal中的块号（Block Number），一个版本号和写入的数据。类似的数组元素会有多个，这样就可以用来描述涉及到修改多份文件系统数据的操作。
- Log会存在工作站的内存中，并尽可能晚的写到Petal中。
- 所以，当工作站从锁服务器收到了一个Revoke消息，要自己释放某个锁，它需要执行多一个步骤。
  - 首先，工作站需要将内存中还没有写入到Petal的Log条目写入到Petal中。
  - 之后，再将被Revoke的Lock所保护的数据写入到Petal。
  - 最后，向锁服务器发送Release消息。

> 故障恢复

- Frangipani对锁使用了租约，租约到期后，锁服务器会认定工作站已崩溃。
- 此时，锁服务器让另一个还活着的工作站读取崩溃站的Log，并重新执行它最近的操作
- 活着的工作站确保这些操作完成后通知锁服务器。
- 锁服务器释放锁。

> 几种故障场景

- 未写入任何Log，也意味着未做修改操作，无需恢复
- 写入部分Log后崩溃，执行恢复的工作站会执行完整的Log条目（这里的隐含意思是需要有类似校验和的机制）
- 写入部分数据后崩溃，执行恢复的工作站会执行全部的Log条目

- 还有更复杂一点的场景，崩溃站的Log在我们执行恢复的时候可能已经过时了，其他的一些工作站可能已经以其他的方式修改了相同的数据，所以我们不能盲目的重新执行Log条目。
  - 引入版本字段，修改操作会将数据的版本号+1，恢复操作的版本号对不上就放弃了。

## 十二、分布式事务

### 12.1 概述

- 分布式事务的两个组成部分：**并发控制(Concurrency Control)** 和 **原子提交(Atomic Commit)**。
- 数据分片在多个服务器，执行事务涉及多个服务器的数据。
- 数据库事务的隔离性原则意味着可序列化/可串行化(Serializable)
  - 并行的执行一些事务得到的结果，与按照某种串行的顺序来执行这些事务，可以得到相同的结果
- 可串行化定义了事务执行过程的正确性
  - 简化了应用程序员的工作，因为无论写的多么复杂，最终都会和以某种顺序串行执行产生一样的效果
  - 如果两个事务不涉及同一个数据，那么它们可以真正意义上并行执行。
  - 对于分布式系统，两台服务器上并行处理事务，能获得更好的并发性能。
- **并发控制就是可序列化的别名**，对使用相同数据的并发事务进行隔离。
- 原子提交：一个事务涉及多台服务器的数据，事务执行过程中可能已经修改了X的值，但涉事的另一台机器故障，需要恢复
  - 哪怕事务涉及的机器只有部分还在运行，我们需要具备能够从部分故障中恢复的能力

### 12.2 并发控制

- 两种策略：悲观并发控制和乐观并发控制
- Pessimistic Concurrency Control：锁冲突会造成延时等待
- Optimistic Concurrency Control：无脑执行读写操作，通常来说这些执行会在一些临时区域。
  - 在事务最后的时候，检查是不是有一些其他的事务干扰。
  - 如果没有，那么你的事务就完成了；
  - 如果有其他的事务在同一时间修改了你关心的数据，并造成了冲突，那么你必须要Abort当前事务，并重试。
- 悲观并发控制 采用两阶段锁。
  - 在读写任何数据前 需要获取 对应的锁。
  - 在事务结束（提交或Abort）前，不允许释放锁。
- 提前释放锁可能会造成 脏读。
  - 读未提交数据（脏读）：A事务读取B事务尚未提交的数据，此时如果B事务发生错误并执行回滚操作，那么A事务读取到的数据就是脏数据。

- 两阶段锁可能会造成死锁。T1：get(X);get(Y);    T2：get(Y);get(X);

### 12.3 两阶段提交

- 如何构建 **分布式事务(Distributed Transaction)**：在一个分布式环境中，数据被分割在多台机器上，如何构建数据库或存储系统以支持事务。
- 需要有一个计算机会用来管理事务，它被称为**事务协调者**（Transaction Coordinator）。
- 在一个完整的分布式系统中，或许会有很多不同的并发运行事务，也会有许多个事务协调者在执行它们各自的事务。在这个架构里的各个组成部分，都需要知道消息对应的是哪个事务。它们都会记录状态。
- 每个持有数据的服务器会维护一个锁的表单，用来记录锁被哪个事务所持有。所以对于事务，需要有事务ID（Transaction ID），简称为TID。
- 每一个消息都被打上TID（事务开始时由事务协调器来分配）作为标记。
- 同时，事务协调器会在本地记录事务的状态，对**事务参与者**（Participants）打上事务ID的标记。
- 简：消息、参与者、锁都会被打上TID。

> 简易流程

- 事务协调者(TC)运行了整个事务，它会向参与者A发送Get请求并得到回复，之后再向参与者B发送一个Put请求并得到回复。
- 当TC到达了事务的结束并想要提交事务，这样才能：
  - 释放所有的锁，
  - 并使得事务的结果对于外部是可见的，
  - 再向客户端回复。

> 详细流程

- 在开始执行事务时，TC需要确保所有的事务参与者能够完成它们在事务中的那部分工作。所以，它会向所有的参与者发送Prepare消息。
- 当参与者收到了Prepare消息，它们就知道事务要执行的内容，它们会查看自身的状态并决定它们实际上能不能完成事务，并回复TC。
  - 或许它们需要**Abort**这个事务因为这个事务会引起**死锁**，或许它们在**故障重启**过程中并完全忘记了这个事务因此不能完成事务。
- 事务协调者会等待来自于每一个参与者的这些Yes/No投票。如果所有的参与者都回复Yes，那么事务可以提交，不会发生错误。
- 之后事务协调者会发出一个Commit消息，给每一个事务的参与者。
- 之后，事务参与者通常会回复ACK说，我们知道了要commit。
- 如果任何一个参与者回复了No，表明自己不能完成这个事务，TC会发送一轮**Abort**消息给所有的参与者说，请撤回这个事务。
- 在事务Commit之后，会发生两件事情：
  - 首先，事务协调者会向客户端发送代表了事务输出的内容，表明事务结束了（事务没有被Abort并且被持久化保存起来了）。
  - 为了遵守两阶段锁，事务参与者会释放锁（这里不论Commit还是Abort都会释放锁）。

### 12.4 故障恢复

#### 12.4.1 参与者故障恢复

> 如果参与者B在Prepare消息之前崩了

- 那么B重启收到Prepare，因为内存数据没了，它不知道事务的任何消息，可能会回TC一个No。事务就Abort掉了。

> 如果参与者B在回复Yes之后，收到commit之前崩了

- 因为这个时候，A收到commit可能就做持久化操作了，所以B在故障恢复后也得接着做。
- 这里就需要**B在回复Prepare之前，将Log（数据、锁列表）写盘**。

> B可能在收到Commit之后崩溃了

- B有可能在处理完Commit之后，发送ACK之前就崩溃了,那么B收到了同一个Commit消息两次。
- 为了节约内存，B会完全忘记已经在磁盘上持久化存储的事务的信息。
- 对于一个它不知道事务的Commit消息，B会简单的ACK这条消息。

#### 12.4.2 协调者故障恢复

> 如果事务协调者在崩溃前没有发送Commit消息，它可以直接Abort事务。

- 因为参与者可以在自己的Log中看到事务，但是又从来没有收到Commit消息，事务的参与者会向事务协调者查询事务，事务协调者会发现自己不认识这个事务，它必然是之前崩溃的时候Abort的事务。

> 如果事务协调者在发送完一个或者多个Commit消息之后崩溃

- 这时，有的参与者可能已经做了持久化操作了，所以事务只能Commit不能Abort。
- **事务协调者在发送任何Commit消息之前，必须先将事务的信息写入到自己的Log，并存放在持久化存储中。**
- 所以，事务协调者在收到所有对于Prepare消息的Yes/No投票后，会将结果和事务ID写入存在磁盘中的Log，之后才会开始发送Commit消息。
- 对于执行了一半的事务，事务协调者会向所有的参与者重发Commit消息或者Abort消息，以防在崩溃前没有向参与者发送这些消息

#### 12.4.3 网络故障、丢包

> TC 发送Prepare消息，但是没收到Yes/No消息

- 一种选择是持续不断的重发Prepare消息，直到收到全部回复。但，这会因为一台机子故障，而阻塞多台机器上的其他事务。
  - 因为这个事务需要的锁，一直被TID持有
- 在事务协调者没有收到Yes/No回复一段时间之后，它可以单方面的Abort事务，并发送一轮Abort消息。
- 如果网络某个地方出现了问题，或者事务协调器挂了一会，事务参与者仍然在等待Prepare消息，总是可以允许事务参与者Abort事务，并释放锁，这样其他事务才可以继续。

> TP 发送完Yes回复，但没有收到commit消息

- 或许网络出现问题了，或许事务协调器的网络连接中断了，或者事务协调器断电了，不管什么原因，B等了很长时间都没有收到Commit消息。
- 如果等待Commit消息超时了，参与者不允许Abort事务，它必须无限的等待Commit消息，这里通常称为Block。
- 因为可能部分参与者已经收到了Commit消息，并做了持久化操作。
- 同时，B也不能单方面决定Commit事务。这样的话，B只能等到故障修复了。

> 事务结束后的一些工作

- 当事务协调者得到了所有的ACK，它可以擦除所有有关事务的记忆。
- 类似的，当一个参与者收到了Commit或者Abort消息，完成了它们在事务中的相应工作，持久化存储事务结果并释放锁，那么在它发送完ACK之后，参与者也可以完全忘记相关的事务。
- 所以，当参与者收到重复Commit时，它不认识该事务，可以容易判断，要么abort了，要么commit了，可以简单回复一个ACK。

### 12.5 总结

- 如果你需要在事务中支持**多条数据**，并且你将**数据分片**在多台服务器之上，那么你必须支持**两阶段提交**。
- 由于Block的存在，参与者需要在持有锁的状态下等待一段长时间，你或许可以在银行内部的系统（小环境）中看见两阶段提交，但永远也不会在物理分隔的不同组织之间看见两阶段提交。

> 和raft浅对照一下

- 都是一个Leader(TC)和多个Follower(TP)的架构
- 不同的是，Follower做的事情一样，TP做的事情不一样。所以，一个是过半票决，一个是全都得参与。
- 也因此，raft是HA的，两阶段提交的可用性很低。

> raft和两阶段提交的结合

- 把TC和TP都复制成3份，TC和相同副本TP都变成一个raft集群。
- 只要过半服务器响应就可以执行指令。
- 这种思想就是分片的数据库，一般会配一个配置管理器。类似结构的系统：Spanner。
